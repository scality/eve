#!/usr/bin/env python
# coding: utf-8
"""Eve configuration file for buildbot.

This module is the core source code of eve.
It is in fact the configuration file for buildbot.
See the `Buildbot Manual`_ for more informations.

.. _Buildbot Manual:
    http://docs.buildbot.net/latest/manual/index.html
"""

import copy
import json
import re
import string
import time
from collections import OrderedDict, defaultdict
from fnmatch import fnmatch
from os import environ, path
from random import randint, choice
from subprocess import Popen, STDOUT, PIPE, CalledProcessError, check_output
from tempfile import mktemp

import docker
import novaclient.exceptions
import yaml
from buildbot.changes.gitpoller import GitPoller
from buildbot.config import BuilderConfig
from buildbot.locks import MasterLock
from buildbot.worker.protocols.pb import Connection
from buildbot.plugins import steps
from buildbot.process import buildrequest, logobserver
from buildbot.process.buildstep import BuildStep
from buildbot.process.factory import BuildFactory
from buildbot.process.properties import Interpolate, Property
from buildbot.process.results import (CANCELLED, EXCEPTION, FAILURE, RETRY,
                                      SKIPPED, SUCCESS, WARNINGS, Results)
from buildbot.reporters.http import HttpStatusPushBase
from buildbot.scheduler import AnyBranchScheduler
from buildbot.schedulers.forcesched import ForceScheduler
from buildbot.schedulers.triggerable import Triggerable
from buildbot.schedulers.trysched import Try_Userpass
from buildbot.steps.http import HTTPStep
from buildbot.steps.master import MasterShellCommand, SetProperty
from buildbot.steps.shell import SetPropertyFromCommand, ShellCommand
from buildbot.steps.source.git import Git
from buildbot.steps.transfer import FileUpload
from buildbot.steps.trigger import Trigger
from buildbot.worker.docker import DockerLatentWorker
from buildbot.worker.local import LocalWorker
from buildbot.worker.openstack import OpenStackLatentWorker
from buildbot.www.authz import Authz, endpointmatchers, roles
from buildbot.www.oauth2 import GoogleAuth
from raven import Client
from raven.transport.twisted import TwistedHTTPTransport
from requests.auth import HTTPBasicAuth
from twisted.internet import reactor, threads
from twisted.internet.defer import (Deferred, inlineCallbacks, returnValue,
                                    succeed)
from twisted.logger import ILogObserver, Logger, globalLogPublisher
from twisted.python.reflect import namedModule
from zope.interface import provider

# Monkey patch to recover quickly when the
Connection.keepalive_interval = 300

MASTER_NAME = environ.pop('MASTER_NAME', 'master')

##########################
# Constants
##########################
BOOTSTRAP_BUILDER_NAME = 'bootstrap-%s' % MASTER_NAME
DOCKER_BUILDER_NAME = 'docker-%s' % MASTER_NAME
OPENSTACK_BUILDER_NAME = 'openstack-%s' % MASTER_NAME
BOOTSTRAP_SCHEDULER_NAME = 'bootstrap-scheduler-%s' % MASTER_NAME
DOCKER_SCHEDULER_NAME = 'docker-scheduler-%s' % MASTER_NAME
OPENSTACK_SCHEDULER_NAME = 'openstack-scheduler-%s' % MASTER_NAME
MAX_LOCAL_WORKERS = int(environ.get('MAX_LOCAL_WORKERS', 8))
MAX_DOCKER_WORKERS = MAX_LOCAL_WORKERS * 6
MAX_OPENSTACK_WORKERS = MAX_LOCAL_WORKERS * 2
EVE_FOLDER = 'eve'
EVE_MAIN_YAML = 'main.yml'
EVE_MAIN_YAML_FULL_PATH = '%s/%s' % (EVE_FOLDER, EVE_MAIN_YAML)
EVE_GIT_POLLING = False

OPENSTACK_IDENTITY_URL = 'https://identity.api.rackspacecloud.com/v2.0/'
OPENSTACK_REGION = environ.get('OPENSTACK_REGION', 'DFW')
OPENSTACK_TENANT = 984990
OPENSTACK_SSH_KEY = path.expanduser(
    environ.get('OPENSTACK_SSH_KEY', '~/.ssh/id_rsa'))
OPENSTACK_KEY_NAME = environ.get('OPENSTACK_KEY_NAME', 'eve-key-pair')

##########################
# Set/Check environment variables
##########################

# store 'secret' environment variables in a separate dictionary
SECRETS = {}

for key in dict(environ):
    if key.startswith('SECRET_'):
        SECRETS[key.lstrip('SECRET_')] = environ.pop(key)

# git
GIT_REPO = environ.pop('GIT_REPO')
GIT_REPO_SHORT = GIT_REPO.split('/')[-1].replace('.git', '')
GIT_CACHE_DIR_HOST = '/tmp/' + GIT_REPO_SHORT
GIT_CACHE_DIR_CONTAINER = '/home/eve/cache/git'
if not GIT_REPO.startswith('git://'):
    # git protocol is anonymous
    GIT_KEY_PATH = environ.pop('GIT_KEY_PATH')
    assert path.isfile(GIT_KEY_PATH), (
        'Did not find git RSA cert in %s' % GIT_KEY_PATH)
    assert path.isfile(GIT_KEY_PATH + '.pub'), (
        'Did not find public git RSA cert in %s.pub' % GIT_KEY_PATH)
# Sentry
SENTRY_DSN = environ.pop('SENTRY_DSN', None)

# docker

EXTERNAL_URL = environ.pop('EXTERNAL_URL')
MASTER_FQDN = environ.pop('MASTER_FQDN')
WORKER_SUFFIX = environ.pop('WORKER_SUFFIX')

DOCKER_HOST = environ.pop('DOCKER_HOST', 'unix:///var/run/docker.sock')
DOCKER_CERT_PATH = environ.pop('DOCKER_CERT_PATH', None)
if DOCKER_CERT_PATH and path.isdir(DOCKER_CERT_PATH):
    DOCKER_CERT_PATH_CA = path.join(DOCKER_CERT_PATH, 'ca.pem')
    DOCKER_CERT_PATH_KEY = path.join(DOCKER_CERT_PATH, 'key.pem')
    DOCKER_CERT_PATH_CERT = path.join(DOCKER_CERT_PATH, 'cert.pem')
    assert path.isfile(DOCKER_CERT_PATH_CA), DOCKER_CERT_PATH_CA
    assert path.isfile(DOCKER_CERT_PATH_KEY), DOCKER_CERT_PATH_KEY
    assert path.isfile(DOCKER_CERT_PATH_CERT), DOCKER_CERT_PATH_CERT
    TLS_CONFIG = docker.tls.TLSConfig(
        client_cert=(
            DOCKER_CERT_PATH_CERT,
            DOCKER_CERT_PATH_KEY),
        ca_cert=DOCKER_CERT_PATH_CA,
        verify=False)
else:
    TLS_CONFIG = None

# bitbucket
EVE_BITBUCKET_LOGIN = environ.pop('EVE_BITBUCKET_LOGIN')
EVE_BITBUCKET_PWD = environ.pop('EVE_BITBUCKET_PWD')

BITBUCKET_PUB_KEY = environ.get('BITBUCKET_PUB_KEY', None)

OAUTH2_CLIENT_ID = environ.pop('OAUTH2_CLIENT_ID')
OAUTH2_CLIENT_SECRET = environ.pop('OAUTH2_CLIENT_SECRET')

PROJECT_NAME = environ.pop('PROJECT_NAME')
PROJECT_URL = environ.pop('PROJECT_URL')

TRY_PWD = environ.pop('TRY_PWD')

# database
DB_URL = environ.pop('DB_URL', 'sqlite:///state.sqlite')

RAX_LOGIN = environ.pop('RAX_LOGIN', None)
RAX_PWD = environ.pop('RAX_PWD', None)

# artifacts
CLOUDFILES_URL = environ.pop(
    'CLOUDFILES_URL',
    'https://storage101.dfw1.clouddrive.com/v1/MossoCloudFS_984990/')
ARTIFACTS_URL = environ.pop(
    'ARTIFACTS_URL',
    'https://artifacts-preprod.devsca.com/builds')

ARTIFACTS_PREFIX = environ.pop('ARTIFACTS_PREFIX', 'staging-')

ARTIFACTS_LOGIN = environ.pop('ARTIFACTS_LOGIN', None)
ARTIFACTS_PWD = environ.pop('ARTIFACTS_PWD', None)

MASTER_START_TIME = time.time()


##########################
# Project Identity
##########################
EVE_CONF = BuildmasterConfig = OrderedDict()  # pylint: disable=invalid-name
EVE_CONF['title'] = "the %s project" % PROJECT_NAME
EVE_CONF['titleURL'] = PROJECT_URL
EVE_CONF['buildbotURL'] = EXTERNAL_URL
EVE_CONF['multiMaster'] = True

# 'protocols' contains information about protocols which master will use for
# communicating with workers. You must define at least 'port' option that
# workers could connect to your master with this protocol.
# 'port' must match the value configured into the buildworkers (with their
# --master option)
EVE_CONF['protocols'] = {'pb': {'port': environ['PB_PORT']}}

##########################
# HipChat Configuration
##########################
REPO_ICON = 'http://www.packal.org/sites/default/files/public/styles/icon_' \
            'large/public/workflow-files/netdeanishealfred-git-repos/icon/' \
            'icon.png?itok=1zkuMgPa'
BRANCH_ICON = 'http://plainicon.com/dboard/userprod/2800_a1826/prod_thumb/' \
              'plainicon.com-50219-512px-201.png'
CLOCK_ICON = 'https://image.freepik.com/free-icon/clock-of-circular-shape-at' \
             '-two-o-clock_318-48022.jpg'

HIPCHAT_TOKEN = environ.pop('HIPCHAT_TOKEN')
HIPCHAT_ROOM = environ.pop('HIPCHAT_ROOM')

##########################
# Web UI
##########################
# Create a basic auth website with the waterfall view and the console view

EVE_CONF['www'] = dict(
    port=environ['HTTP_PORT'],
    plugins=dict(waterfall_view={}, console_view={}),
    auth=GoogleAuth(OAUTH2_CLIENT_ID,
                    OAUTH2_CLIENT_SECRET),
    change_hook_dialects={'bitbucket': True},
)

# Limit write operations to the OAUTH2_CLIENT_ID account except for tests


class DeveloperRoleIfConnected(roles.RolesFromBase):
    """ Sets the 'developer' role to all authenticated users"""

    def getRolesFromUser(self, userDetails):
        if 'email' in userDetails:  # and userDetails['email']:
            return ['developer']
        return []

if OAUTH2_CLIENT_ID != 'test':
    EVE_CONF['www']['authz'] = Authz(
        allowRules=[
            endpointmatchers.AnyEndpointMatcher(role='developer'),
        ],
        roleMatchers=[
            DeveloperRoleIfConnected()
        ]
    )

# DB URL
EVE_CONF['db'] = {
    'db_url': DB_URL,
}

# #########################
# Ngrok
# #########################

class NgrokNotAvailableError(Exception):
    """ngrok binary could not be found."""
    pass

class NgrokTimeoutError(Exception):
    """ngrok took too much time to create the tunnel."""
    pass

class Ngrok(object):
    """ngrok utility wrapper"""

    MAX_WAIT_TUNNEL = 5
    URL_REGEXP = re.compile(r'URL:\w+://(\S+):(\d+)')

    def __init__(self, command='ngrok'):
        super(Ngrok, self).__init__()
        self.command = path.expanduser(command)
        self._proc = None
        self._state = None

    def start(self, protocol, port, region='us'):
        """Start ngrok and returns tunnel url and port.

        Args:
            protocol: str: ngrok tunnel protocol on host
            port: str: ngrok tunel port on host
            region: str: --region ngrok option

        Returns: tuple (str, str) : (url, port)

        Raises:
            * NgrokNotAvailableError
            * NgrokTimeoutError

        """
        command = [
            self.command, str(protocol), str(port),
            '--log', 'stdout', '--log-level', 'debug', '--log-format', 'json',
            '--region', region
        ]

        try:
            proc = Popen(command, stdout=PIPE)
        except OSError as error:
            raise NgrokNotAvailableError(
                "Couldn't start ngrok with command: '%s' : '%s'."
                "Is Ngrok in the PATH ?" %
                (command, error)
            )

        start = time.time()
        while not proc.poll() and time.time() - start < self.MAX_WAIT_TUNNEL:
            stdout = proc.stdout.readline()
            if not stdout:
                time.sleep(0.1)
                continue
            data = json.loads(stdout)
            if 'resp' in data:
                match = self.URL_REGEXP.search(data["resp"])
                if match:
                    self._proc = proc
                    self._state = (match.group(1), match.group(2))
                    return self._state

        # we reached the MAX_WAIT_TUNNEL timeout. end process and raise error
        proc.terminate()
        raise NgrokTimeoutError(
            'Ngrock took more than the %s max seconds'
            ' to return tunnel specifications. Aborted.' %
            self.MAX_WAIT_TUNNEL)

    @property
    def running(self):
        """Returns: boolean: True if ngrok is running."""
        return self._state != None

    def stop(self):
        """Stop ngrok, if started."""
        if self.running:
            self._proc.terminate()
            self._proc = None
            self._state = None

    def __del__(self):
        self.stop()


# #########################
# Reporters
# #########################
# Reporters send the build status when finished

class BaseBuildStatusPush(HttpStatusPushBase):
    """
    Base class for pushing build status
    """
    neededDetails = dict(wantProperties=True, wantSteps=True)
    RESULT_COLOR_CORRESP = {
        SUCCESS: 'green',
        WARNINGS: 'orange',
        FAILURE: 'red',
        SKIPPED: 'white',
        EXCEPTION: 'purple',
        RETRY: 'purple',
        CANCELLED: 'pink'}

    def gather_data(self, build):
        """
        Gathers data to be used in build status
        :param build: The build dictionary
        :return: (key, result, title, summary, description)
        """
        key = 'pre-merge'  # fixme: this should be set dynamically
        src = build['buildset']['sourcestamps'][0]
        repo = GIT_REPO_SHORT
        branch = src['branch']
        title = 'build #%s' % build['buildid']
        summary = '(%s) build #%s on %s:%s ' % (
            build['state_string'], build['buildid'], repo, branch)

        self.add_tag('branch', branch, BRANCH_ICON, color='blue')
        self.add_tag('repository', repo, REPO_ICON, color='blue')

        result = build['results']
        description = 'in progress...'
        if result is not None:
            description = 'Hooray!'
            duration = (build['complete_at'] - build['started_at']).\
                total_seconds()
            summary += '[%s]' % Results[result]
            if result != SUCCESS:
                description = Results[result] + ' in step(s) : ' + ', '.join([
                    l['name'] for l in build['steps']
                    if l['results'] == build['results']])

            self.add_tag('result', Results[result], None,
                         color=self.RESULT_COLOR_CORRESP[result])
            self.add_tag('duration', '%d seconds' % duration, CLOCK_ICON,
                         color='gray')
        # fixme: add authors
        # authors = build.getResponsibleUsers()
        return key, result, title, summary, description

    def add_tag(self, name, value, icon, color=None):
        """
        Add a tag (name+value) to the status
        :param name: The name of the tag
        :param value: The value of the tag
        :param icon: a square image url (can be None) (HipChat Only)
        :param color: The color of the tag (HipChat Only)
        :return: None
        """
        raise NotImplementedError()


class HipChatBuildStatusPush(BaseBuildStatusPush):
    """Send build result to HipChat build status API."""
    name = "HipChatBuildStatusPush"
    logger = Logger('eve.steps.HipChatBuildStatusPush')
    attributes = []
    COLOR_STYLE_CORRESP = {
        'green': 'lozenge-success',
        'orange': 'lozenge-current',
        'red': 'lozenge-error',
        'white': 'lozenge',
        'purple': 'lozenge-error',
        'pink': 'lozenge-error',
        'brown': 'lozenge-moved',
        'blue': 'lozenge-complete',
        'gray': 'lozenge'}
    HIPCHAT_COLOR_CORRESP = {
        SUCCESS: 'green',
        WARNINGS: 'yellow',
        FAILURE: 'red',
        SKIPPED: 'gray',
        EXCEPTION: 'purple',
        RETRY: 'purple',
        CANCELLED: 'gray',
        None: 'gray'}

    def add_tag(self, name, value, icon, color=None):
        attr = dict(label=name, value=dict(label=value))
        if color in self.COLOR_STYLE_CORRESP:
            attr['value']['style'] = self.COLOR_STYLE_CORRESP[color]
        if icon:
            attr['value']['icon'] = dict(url=icon)
        self.attributes.append(attr)

    @inlineCallbacks
    def send(self, build):
        """Send build status to HipChat."""

        self.attributes = []
        key, result, title, summary, description = self.gather_data(build)

        headers = {
            'content-type': 'application/json',
            'authorization': 'Bearer %s' % HIPCHAT_TOKEN}

        card = dict(
            style='application',
            url=build['url'],
            format='medium',
            id=key,
            title=title,
            description=dict(format='text', value=description),
            attributes=self.attributes,
            activity=dict(html=summary))

        data = dict(
            message=summary,
            name=key,
            message_format='text',
            notify=True,
            card=card,
            color=self.HIPCHAT_COLOR_CORRESP[result])

        url = 'https://api.hipchat.com/v2/room/%s/notification' % HIPCHAT_ROOM

        if EVE_BITBUCKET_LOGIN == 'test':
            return  # Don't really push status for tests
        response = yield self.session.post(url, headers=headers, json=data)
        if response.status_code != 204:
            raise Exception(
                "{response.status_code}: unable to send status to HipChat: "
                "{url}\nRequest:\n{request}\nResponse:\n{response.content}".
                    format(request=data, response=response, url=url))
        self.logger.info("HipChat status sent")

# The status push works only on the main builder (bootstrap)
EVE_CONF['services'] = [HipChatBuildStatusPush(builders=[BOOTSTRAP_BUILDER_NAME])]


class BitbucketBuildStatusPush(BaseBuildStatusPush):
    """Send build result to bitbucket build status API."""
    name = "BitbucketBuildStatusPush"
    description_suffix = ''
    logger = Logger('eve.steps.BitbucketBuildStatusPush')
    BITBUCKET_STATUS_CORRESP = {
        SUCCESS: 'SUCCESSFUL',
        WARNINGS: 'SUCCESSFUL',
        FAILURE: 'FAILED',
        SKIPPED: 'FAILED',
        EXCEPTION: 'FAILED',
        RETRY: 'FAILED',
        CANCELLED: 'FAILED',
        None: 'INPROGRESS'}

    @staticmethod
    def forge_url(build):
        """Forge the BB API URL on which the build status will be posted."""
        sha1 = build['buildset']['sourcestamps'][0]['revision']
        return 'https://api.bitbucket.org/2.0/repositories/' \
               '%(repo_owner)s/%(repo_name)s/commit/%(sha1)s/statuses/build' \
               % {
                   'repo_owner': 'scality',
                   'repo_name': GIT_REPO_SHORT,
                   'sha1': sha1
               }

    def add_tag(self, name, value, icon, color=None):
        name_value = '[%s: %s]' % (name, value)
        self.description_suffix = name_value + self.description_suffix

    @inlineCallbacks
    def send(self, build):
        """Send build status to Bitbucket."""
        self.description_suffix = ''
        key, result, _, summary, description = self.gather_data(build)
        data = {
            'state': self.BITBUCKET_STATUS_CORRESP[result],
            'key': key,
            "name": summary,
            "url": build['url'],
            "description": description + self.description_suffix
        }
        auth = HTTPBasicAuth(EVE_BITBUCKET_LOGIN, EVE_BITBUCKET_PWD)
        url = self.forge_url(build)
        if 'eve.devsca.com' not in EXTERNAL_URL:
            self.logger.info("Bitbucket status not sent (not in prod)")
            return  # Don't really push status for tests
        response = yield self.session.post(
            url, json=data, auth=auth)
        # 200 means that the key already exists
        # 201 means that the key has been created successfully
        if response.status_code not in (200, 201):
            raise Exception(
                "{response.status_code}: unable to send status to Bitbucket: "
                "{url}\nRequest:\n{request}\nResponse:\n{response.content}".
                    format(request=data, response=response, url=url))
        self.logger.info("Bibucket status sent")

# The status push works only on the main builder (bootstrap)
EVE_CONF['services'].append(BitbucketBuildStatusPush(
    builders=[BOOTSTRAP_BUILDER_NAME]))

##########################
# ArtifactUpload
##########################

CURL_CMD = """curl -s -X POST -H "Content-type: application/json" \
--progress-bar https://identity.api.rackspacecloud.com/v2.0/tokens \
-d '{ \
        "auth": { \
            "passwordCredentials": { \
                "username": "'$RAX_LOGIN'", \
                "password": "'$RAX_PWD'" \
            } \
        } \
    }'
"""


class CloudfilesAuthenticate(SetPropertyFromCommand):
    """ Authenticate with rackspace and store the auth token on a property."""

    def __init__(self, **kwargs):
        SetPropertyFromCommand.__init__(
            self,
            name='get cloudfiles authentication params',
            command=CURL_CMD,
            property='cloudfiles_token',
            haltOnFailure=True,
            env={'RAX_LOGIN': RAX_LOGIN, 'RAX_PWD': RAX_PWD},
            logEnviron=False,  # Obfuscate $RAX_PWD
            **kwargs
        )

    def commandComplete(self, cmd):  # NOQA flake8 to ignore camelCase
        if cmd.didFail():
            return
        output = json.loads(self.observer.getStdout())
        token = output["access"]["token"]["id"]
        self.setProperty(self.property, str(token), "CloudfilesAuthenticate")
        self.property_changes[self.property] = token


class Upload(ShellCommand):
    """ Upload files to rackspace
    """
    def __init__(self, source, urls=None, **kwargs):
        name = kwargs.pop('name', 'send artifacts to artifact repository')
        self._retry = kwargs.pop('retry', (0, 1))
        self._source = source
        self._kwargs = kwargs

        command = [
            'cd ' + source,
            ('[ "$(ls -A)" ]'
             ' || (echo "Directory is empty. Nothing to do."; exit 1)'),
            'tar -chvzf ../artifacts.tar.gz . ',
            'echo tar successful. Calling curl... ',
            ('curl --verbose --retry 10 -s -T ../artifacts.tar.gz '
             '-X PUT -H"x-auth-token: %(prop:cloudfiles_token)s" ' +
             CLOUDFILES_URL + ARTIFACTS_PREFIX + '%(prop:build_id)s' +
             '?extract-archive=tar.gz'),
        ]

        # compute configured urls
        links = []
        if urls:
            for path in urls:
                if isinstance(path, tuple) or isinstance(path, list):
                    link = {'name': path[0], 'path': path[1]}
                else:
                    link = {'path': path}
                link['header'] = 'find files matching {path}:'.format(path=link['path'])
                links.append(link)

        for link in links:
            command.append(
                'echo -e "\n{header}\n$(find . -type f -path \'./{path}\')\n\n"'.format(
                    header=link['header'],
                    path=link['path']
            ))
        self._links = links

        ShellCommand.__init__(
            self,
            name=name,
            haltOnFailure=True,
            command=Interpolate(' && '.join(command)),
            **kwargs
        )
        self.observer = logobserver.BufferLogObserver(wantStdout=True,
                                                      wantStderr=True)
        self.addLogObserver('stdio', self.observer)

    @inlineCallbacks
    def run(self):
        result = yield super(Upload, self).run()
        if result == FAILURE:
            delay, repeats = self._retry
            if repeats > 0:
                # Wait for delay before retrying
                sleep_df = Deferred()
                reactor.callLater(delay, sleep_df.callback, None)
                yield sleep_df

                # Schedule a retry after this step
                self.build.addStepsAfterCurrentStep([self.__class__(
                    source=self._source,
                    urls=self._urls,
                    retry=(delay, repeats - 1),
                    **self._kwargs)])
                returnValue(SKIPPED)
        returnValue(result)

    def evaluateCommand(self, cmd):
        out = self.observer.getStdout()
        err = self.observer.getStderr()
        if ('Cowardly refusing to create an empty archive' in err
                or 'No such file or directory' in err
                or 'File removed before we read it' in err
                or 'Directory is empty' in out):
            return SKIPPED
        elif 'Response Status: 201 Created' not in out:
            return FAILURE
        return cmd.results()

    # regexp use to make the difference between simple link prefix and link
    # name patterns
    PREFIX_PATTERN_ELEMENTS = re.compile(r'\\\d+')

    def commandComplete(self, cmd):
        # if command failed or was skipped, no need to publish urls
        if self.evaluateCommand(cmd) in [SKIPPED, FAILURE]:
            return

        # extracts file path used in urls, in command output:
        lines = self.observer.getStdout().split('\n')
        links = set()
        for link in self._links:
            matches = []
            start = lines.index(link['header'])
            for line in lines[start+1:]:
                if line == '':
                    break
                matches.append(line)

            if not matches:
                continue
            if (len(matches) == 1
                and '*' not in link['path'] and '?' not in link['path']
                and 'name' in link) :
                links.add((link['name'], matches[0]))
            else:
                prefix = link['name'] if 'name' in link else ''

                # case 1 : prefix is a pattern
                if self.PREFIX_PATTERN_ELEMENTS.search(prefix):
                    path_pattern = re.compile(
                        '^' +
                        link['path'].replace('*', '(.*)').replace('?', '(.)') +
                        '$')
                    links.update([
                        (path_pattern.sub(prefix, match), match)
                        for match in matches
                    ])

                # case 2 : prefix is a prefix
                else:
                    links.update([
                        ('{prefix}{name}'.format(
                            prefix=prefix, name=match.split('/')[-1]),
                         match)
                        for match in matches
                    ])

        links = sorted(links)

        # append path to names if several links share the same name
        pathes = defaultdict(int)
        for name, path in links:
            pathes[name] += 1
        for index, (name, path) in enumerate(links):
            if pathes[name] > 1:
                links[index] = (
                    '{name} ({detail})'.format(name=name, detail=path[2:]),
                    path)

        # adds links in step display, based on existing files
        for (name, path) in links:
            url = ('{url}/{prefix}{build_id}/{path}'.format(
                url=ARTIFACTS_URL,
                prefix=ARTIFACTS_PREFIX,
                build_id=self.getProperty('build_id'),
                path=path
            ))
            self.addURL(name, url)


class Download(ShellCommand):

    def __init__(self, files, **kwargs):
        name = kwargs.pop('name', 'downloads artifacts from artifact repository')
        self._retry = kwargs.pop('retry', (0, 1))
        self._kwargs = kwargs

        command = []
        for fpath in files:
            url = '{url}/{prefix}%(prop:build_id)s'.format(
                url=ARTIFACTS_URL,
                prefix=ARTIFACTS_PREFIX)

            # inserts artifacts login / pwd in url
            url = url.replace(
                'https://',
                'https://{login}:{password}@'.format(
                    login=ARTIFACTS_LOGIN,
                    password=ARTIFACTS_PWD
                ));

            output = fpath

            command.append(
                'echo "> download {path} from artifacts into {output}"'.format(
                    path=fpath,
                    output=output
                ))

            output_dir = path.dirname(fpath)
            if output_dir:
                command.append(
                    'mkdir -p {output_dir}'.format(output_dir=output_dir))

            command.append(
                'curl --silent --retry 10 -s --fail '
                '--output {output} '
                '{url}/{path}'.format(url=url, path=fpath, output=output))

        ShellCommand.__init__(
            self,
            name=name,
            haltOnFailure=True,
            command=Interpolate(' && '.join(command)),
            **kwargs
        )
        self.observer = logobserver.BufferLogObserver(wantStdout=True,
                                                      wantStderr=True)
        self.addLogObserver('stdio', self.observer)


class ShellCommandWithSecrets(ShellCommand):
    """ Execute a shell command that needs secret environment variables.

    All variables on the form SECRET_{var} will be passed as {var} inside the
    worker. Naturally, the environment is not logged during such a step.

    """
    def __init__(self, workdir=None, command=None, usePTY=None, **kwargs):
        kwargs['logEnviron'] = False
        kwargs['env'] = {}
        kwargs['env'].update(environ)
        kwargs['env'].update(SECRETS)
        ShellCommand.__init__(self, workdir, command, usePTY, **kwargs)


##########################
# Workers
##########################
EVE_CONF['workers'] = []


def password_generator(size=18, chars=string.ascii_uppercase + string.digits):
    return ''.join(choice(chars) for _ in range(size))

# Create MAX_LOCAL_WORKERS Local Workers that will bootstrap all the jobs
LOCAL_WORKERS = []
for i in range(MAX_LOCAL_WORKERS):
    LOCAL_WORKERS.append(LocalWorker('lw%03d-%s' % (i, WORKER_SUFFIX)))
EVE_CONF['workers'].extend(LOCAL_WORKERS)


class EveDockerLatentWorker(DockerLatentWorker):
    """Improved version of DockerLatentWorker using the docker command line
     client instead of docker-py which was the cause of multiple dead locks
    """
    logger = Logger('eve.workers.EveDockerLatentWorker')

    def _thd_start_instance(self, image, volumes):
        if image not in self.docker_invoke('images'):
            # hack to avoid a loop when the original image does not exist
            self.docker_invoke('pull', 'ubuntu:trusty')
            image = 'ubuntu:trusty'

        cmd = [
            'run',
            '--privileged',
            '--env', 'BUILDMASTER=%s' % self.masterFQDN,
            '--env', 'WORKERNAME=%s' % self.name,
            '--env', 'WORKERPASS=%s' % self.password,
            '--env', 'BUILDMASTER_PORT=%s' % environ['PB_PORT'],
            '--env', 'DOCKER_HOST_IP=%s' % DOCKER_HOST,
            '--detach',
        ]
        cmd.extend(['--volume=%s' % volume for volume in volumes])
        cmd.append(image)
        self.instance = self.docker_invoke(*cmd)
        self.logger.debug('Container created, Id: %s...' % self.instance)
        return [self.instance, image]

    def _thd_stop_instance(self, instance, fast):
        self.logger.debug('Stopping container %s...' % instance)
        self.docker_invoke('kill', instance)
        self.docker_invoke('wait', instance)
        self.docker_invoke('rm', instance)
        self.logger.debug('Container %s stopped successfully.' % instance)

    def docker_invoke(self, *args):
        """calls the docker client binary with the arguments given as a
         parameter and logs exceptions if any.
         Returns the output of the commmand (stderr + stdout)

         """
        cmd = ['docker']
        cmd.extend(args)
        try:
            res = check_output(cmd, stderr=STDOUT).strip()
            return res
        except CalledProcessError as exception:
            time.sleep(5)  # avoid a fast loop in case of failure
            self.logger.debug('Error: command %s failed: %s' %
                              (cmd, exception.output))
            raise


# Then create MAX_DOCKER_WORKERS Docker Workers that will do the real job
DOCKER_WORKERS = []
for i in range(MAX_DOCKER_WORKERS):
    DOCKER_WORKERS.append(
        EveDockerLatentWorker(
            name='dw%03d-%s' % (i, WORKER_SUFFIX),
            password=password_generator(),
            docker_host=DOCKER_HOST,
            hostconfig={},
            volumes=Property('docker_volumes'),
            tls=TLS_CONFIG,
            image=Property('docker_image'),
            followStartupLogs=True,
            keepalive_interval=300,
            masterFQDN=MASTER_FQDN))
EVE_CONF['workers'].extend(DOCKER_WORKERS)


class EveOpenStackLatentWorker(OpenStackLatentWorker):
    """Improved version of OpenStackLatentWorker that adds:
    - Support for regions (Required for Rackspace)
    - Automatically installs a buildbot slave after spawn using ssh
    """
    logger = Logger('eve.EveOpenStackLatentWorker')

    # (address, port) tuple used to configure buildbot-worker on VM
    _reachable_address = None

    # Ngrok instance, if used (if env var NGROK is set)
    _ngrok = None

    @classmethod
    def get_reachable_address(cls):
        if not cls._reachable_address:
            if 'NGROK' in environ:
                cls._ngrok = Ngrok(command=environ['NGROK'])
                cls._reachable_address = cls._ngrok.start('tcp', environ['PB_PORT'], 'us')
            else:
                cls._reachable_address = MASTER_FQDN, environ['PB_PORT']
        return cls._reachable_address

    def __init__(self, **kwargs):
        OpenStackLatentWorker.__init__(self, **kwargs)
        # fixme: This is a fragile hack because the original class does not
        # allow to specify a region name. We should fix this upstream.
        self.novaclient.client.region_name = OPENSTACK_REGION
        self.ip_address = None
        self._ngrok = None

    def ssh(self, cmd):
        """ Execute an ssh command on the instance.
        :param cmd: The command to launch
        :return: the output of the command
        """
        self.logger.debug('Executing "%s" on %s %s' %
                          (cmd, self.workername, self.ip_address))
        res = check_output(
            'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no '
            '-o SendEnv=LC_ALL -i %s root@%s \'%s\'' % (
                OPENSTACK_SSH_KEY, self.ip_address, cmd),
            shell=True, stderr=STDOUT)
        return res

    def scp(self, src, dst):
        """
        Send files to instance using scp
        :param src: the source file
        :param dst: the destination file
        :return:
        """
        self.logger.debug('Copying %s to on %s %s:%s ' %
                          (src, self.ip_address, dst, self.workername))
        res = check_output(
            'scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no '
            '-o SendEnv=LC_ALL -i %s %s root@%s:%s' % (
                OPENSTACK_SSH_KEY, src, self.ip_address, dst),
            shell=True, stderr=STDOUT)
        return res

    def ssh_ping(self):
        """Dummy command to test ssh connexion."""
        self.ssh('ls')

    def _start_instance(self):
        result = OpenStackLatentWorker._start_instance(self)
        if not self.instance:
            return result

        inst = self.novaclient.servers.get(self.instance.id)
        for network in inst.networks[u'public']:
            if re.match(r'\d+\.\d+\.\d+\.\d+', network):
                self.ip_address = network
                break
        else:
            assert False, 'Could not extract IP address'

        for _ in range(30):
            time.sleep(2)
            try:
                self.ssh_ping()
                break
            except CalledProcessError as exception:
                self.logger.debug('Pinging host %s %s <%s> %s. Retrying...' % (
                    self.workername, self.ip_address,
                    exception, exception.output))

        try:
            self.start_worker()
        except CalledProcessError as exception:
            self.logger.debug('Error on %s %s while executing "%s" <%s> %s.' %
                              (self.workername, self.ip_address,
                               exception.cmd, exception, exception.output))
            time.sleep(60)  # avoid a fast loop in case of failure
            raise
        return result

    def start_worker(self):
        """
        install ssh keys and the buildbot worker and start it.
        :return:
        """
        self.ssh('apt-get update')
        self.ssh('apt-get install -y python-dev python-pip wget git')
        self.ssh('pip install buildbot-worker==0.9.0rc2')
        self.ssh('pip install service_identity')
        self.ssh('adduser -u 1042 --home /home/eve --disabled-password '
                 '--gecos "" eve')
        self.ssh('adduser eve sudo')
        self.ssh('echo "%sudo ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers')
        self.ssh('mkdir /home/eve/.ssh')
        self.scp(GIT_KEY_PATH, '/home/eve/.ssh/id_rsa')
        self.scp(GIT_KEY_PATH + '.pub', '/home/eve/.ssh/id_rsa.pub')
        self.ssh('chown eve:eve /home/eve/.ssh/id_rsa* && '
                 'chmod 600 /home/eve/.ssh/id_rsa')
        assert BITBUCKET_PUB_KEY
        self.ssh('echo "%s" >> ' % BITBUCKET_PUB_KEY +
                 '/home/eve/.ssh/known_hosts && '
                 'chown eve:eve /home/eve/.ssh/known_hosts && '
                 'chmod 644 /home/eve/.ssh/known_hosts')

        master, port = EveOpenStackLatentWorker.get_reachable_address()
        self.ssh('sudo -u eve buildbot-worker create-worker --umask=022 '
                 '/home/eve/worker %s:%s %s "%s"' %
                 (master, port, self.name, self.password))

        self.ssh('sudo -u eve buildbot-worker start /home/eve/worker')

    def buildFinished(self, sb):
        super(EveOpenStackLatentWorker, self).buildFinished(sb)

        # This is a hack to avoid the bug of vms staying in 'preparing worker'
        # for hours
        # what happens is : we setup the openstack VM in start_worker called in
        # start_instance, which is called by substantiate, only if self.conn
        # is None (seems to be intended to be called one time only per worker,
        # whereas our start_instance is written to run 1 time per VM)
        if self.conn:
            self.conn.detached(None)

    def stop_instance(self, fast=False):

        if self.instance is None:
            # be gentle.  Something may just be trying to alert us that an
            # instance never attached, and it's because, somehow, we never
            # started.
            return succeed(None)
        instance = self.instance

        # this allows to call the vm deletion in a thread so we can wait
        # until we are sure they are deleted (Not the case in the original
        # class)
        threads.deferToThread(self._stop_instance, instance, fast)
        self.output = self.instance = self.conn = None

    def _stop_instance(self, instance, fast):
        try:
            inst = self.novaclient.servers.get(instance.id)
        except novaclient.exceptions.NotFound:
            # If can't find the instance, then it's already gone.
            self.logger.info('%s %s instance %s (%s) does not exist' %
                (self.__class__.__name__, self.workername, instance.id,
                 instance.name))
            return

        if inst.status in ('DELETED', 'UNKNOWN'):
            self.logger.info('%s %s instance %s (%s) already deleted' %
                (self.__class__.__name__, self.workername, instance.id,
                 instance.name))
            return

        inst.delete()
        while inst.status == 'ACTIVE':
            try:
                time.sleep(10)
                inst.get()
                self.logger.info('%s %s instance %s (%s) waiting for '
                                 'deletion' %
                    (self.__class__.__name__, self.workername, instance.id,
                     instance.name))
            except novaclient.exceptions.NotFound:
                break

        self.logger.info('%s %s instance %s (%s) deleted successfully' %
                         (self.__class__.__name__, self.workername,
                          instance.id, instance.name))


def get_image_by_name(images):
    """
    Allows to select openstack images based on their names rather than their
     uuids.
    :param images: a list of nova image objects
    :return: the image that matches the name
    """
    for image in images:
        if image.name == 'Ubuntu 14.04 LTS (Trusty Tahr) (PVHVM)':
            return image

    raise Exception('Die Hard!')


# Then create MAX_OPENSTACK_WORKERS Docker Workers that will do the real job
OPENSTACK_WORKERS = []
for i in range(MAX_OPENSTACK_WORKERS):
    OPENSTACK_WORKERS.append(
        EveOpenStackLatentWorker(
            name='ow%03d-%s' % (i, WORKER_SUFFIX),
            password=password_generator(),
            image=get_image_by_name,
            # flavor=Property('openstack_flavor'),
            flavor='general1-8',
            block_devices=None,
            os_auth_url=OPENSTACK_IDENTITY_URL,
            os_tenant_name=OPENSTACK_TENANT,
            os_username=RAX_LOGIN,
            os_password=RAX_PWD,
            meta=None,
            nova_args=dict(key_name=OPENSTACK_KEY_NAME),
            build_wait_timeout=0,  # do not reuse the instance
            keepalive_interval=300,
            client_version='2'))
EVE_CONF['workers'].extend(OPENSTACK_WORKERS)

##########################
# Change Sources
##########################
# the 'change_source' setting tells the buildmaster how it should find out
# about source code changes.

EVE_CONF['change_source'] = []
if EVE_GIT_POLLING:
    EVE_CONF['change_source'].append(GitPoller(
        GIT_REPO,
        workdir='gitpoller-workdir',
        branches=True,
        pollinterval=900,
        pollAtLaunch=False,
        buildPushesWithNoCommits=True,
    ))


##########################
# Custom Build Steps
##########################
class ReadConfFromYaml(FileUpload):
    """Load the YAML file to `conf` property.

    This step Reads the YAML file and converts it to a `conf` property which
    is made available to the following steps.
    """
    logger = Logger('eve.steps.ReadConfFromYaml')

    def __init__(self, **kwargs):
        self.masterdest = mktemp()
        FileUpload.__init__(
            self,
            name='read %s' % EVE_MAIN_YAML_FULL_PATH,
            workersrc=EVE_MAIN_YAML_FULL_PATH,
            masterdest=self.masterdest,
            haltOnFailure=True,
            **kwargs)

    @inlineCallbacks
    def run(self):
        result = yield FileUpload.run(self)
        if result != SUCCESS:
            self.addCompleteLog('stderr', 'Could not find %s' %
                                EVE_MAIN_YAML_FULL_PATH)
            returnValue(result)

        raw_conf = open(self.masterdest).read()
        self.addCompleteLog(EVE_MAIN_YAML_FULL_PATH, raw_conf)
        conf = yaml.load(raw_conf)
        # Expand entries with several branch patterns
        try:
            branches = conf['branches']
        except (TypeError, KeyError):
            self.addCompleteLog('stderr', 'Could not find the branches field'
                                          'in %s' % EVE_MAIN_YAML_FULL_PATH)
            returnValue(FAILURE)

        new_branches = {}
        for branch_patterns, branch_conf in branches.items():
            for branch_pattern in branch_patterns.split(','):
                new_branches[branch_pattern.strip()] = branch_conf
        conf['branches'] = new_branches
        self.setProperty('conf', conf, "ReadConfFromYaml")
        self.setProperty('master_start_time', MASTER_START_TIME)

        # Find the stage name from the branch name
        branch = self.getProperty('branch')
        if branch is None:
            branch = 'default'
        for branch_pattern, branch_conf in conf['branches'].items():
            self.logger.debug('Checking if <{branch}> matches <{pattern}>',
                              branch=branch, pattern=branch_pattern)
            if fnmatch(branch, branch_pattern):
                stage_name = branch_conf['stage']
                self.logger.debug('<{branch}> matched <{branch_pattern}>',
                                  branch=branch, branch_pattern=branch_pattern)
                break
        else:
            self.logger.debug('No branch match. Using default branch config.')
            try:
                stage_name = conf['branches']['default']['stage']
            except KeyError:
                self.addCompleteLog(
                    'stderr', 'Branch <%s> not covered by yaml file' % branch)
                returnValue(CANCELLED)

        self.setProperty('stage_name', stage_name, 'ReadConfFromYaml Step')

        self.build.addStepsAfterCurrentStep([
            TriggerStages([stage_name], haltOnFailure=True)])
        returnValue(SUCCESS)


class CancelCommand(MasterShellCommand):
    """Cancel a build according to result of command."""

    def processEnded(self, status_object):
        """If the return code is non-zero set build to CANCELLED."""
        if status_object.value.exitCode != 0:
            self.finished(CANCELLED)
        else:
            super(CancelCommand, self).processEnded(status_object)


class CancelNonTipBuild(CancelCommand):
    """Cancel if the current revision is not the tip of the branch."""

    def __init__(self, **kwargs):
        super(CancelNonTipBuild, self).__init__(
            name='check if build is relevant',
            command=Interpolate('[ "$(git rev-list -1 %(prop:branch)s)" = '
                                '"%(prop:revision)s" ]'),
            workdir=GIT_CACHE_DIR_HOST,
        )


class CancelOldBuild(CancelCommand):
    """Cancel if the build is previous buildbot instance."""

    def __init__(self, **kwargs):
        super(CancelOldBuild, self).__init__(
            name='prevent unuseful restarts',
            command=Interpolate('[ "' + str(MASTER_START_TIME) +
                                '" = "%(prop:master_start_time)s" ]'),
        )


class StepExtractor(BuildStep):
    """Extracts and adds the build steps to the current builder.

    This step extracts the build steps from the `conf` property and adds them
    to the current builder. It also adds a step to build an image.
    """
    name = 'step extractor'
    logger = Logger('eve.steps.StepExtractor')

    def run(self):
        conf = self.getProperty('conf')
        stage_name = self.getProperty('stage_name')
        stage_conf = conf['stages'][stage_name]
        for step in copy.deepcopy(stage_conf['steps']):
            step_type, params = dict.popitem(step)
            try:
                # try to see if the required step is imported or
                # defined in the current context
                _cls = globals()[step_type]
            except KeyError:
                # otherwise import the step from standars buildbot steps
                try:
                    _cls = getattr(steps, step_type)
                except AttributeError:
                    raise Exception('Could not load step %s' % step_type)

            # Replace the %(prop:*)s in the text with an Interpolate obj
            params = replace_with_interpolate(params)

            if issubclass(_cls, (Git, Upload)):
                # retry 10 times if git or upload step fails, wait 60s
                # between retries
                params['retry'] = (60, 10)

            # hack to avoid putting clear passwords into the YAML file
            # for the HTTP step
            if issubclass(_cls, HTTPStep):
                pwd = params['auth'][1].replace('$', '')
                if pwd in environ:
                    params['auth'] = HTTPBasicAuth(
                        params['auth'][0], environ[pwd])

            # Hack! Buildbot does not accept unicode step names
            if 'name' in params and isinstance(params['name'], unicode):
                params['name'] = params['name'].encode('utf-8')

            step = _cls(**params)
            self.build.addStepsAfterLastStep([step])
            self.logger.debug('Add {step} with params : {params}',
                              step=step_type, params=params)
        return succeed(SUCCESS)


class TriggerStages(BuildStep):
    """Start a list of stages."""

    def __init__(self, stage_names, **kwargs):
        self.stage_names = stage_names
        BuildStep.__init__(self, haltOnFailure=True)
        self.kwargs = kwargs
        if 'waitForFinish' not in kwargs:
            kwargs['waitForFinish'] = True
        self.name = 'prepare %d stage(s)' % len(stage_names)

    def run(self):
        conf = self.getProperty('conf')
        preliminary_steps = []
        build_orders = []

        for stage_name in reversed(self.stage_names):
            worker_type = conf['stages'][stage_name]['worker']['type']
            assert worker_type in ('docker', 'openstack')
            if worker_type == 'docker':
                build_order = DockerBuildOrder(stage_name, conf, self)
            elif worker_type == 'openstack':
                # pylint bug 710
                # pylint: disable=redefined-variable-type
                build_order = OpenStackBuildOrder(stage_name, conf, self)
            else:
                return succeed(FAILURE)
            build_orders.append(build_order)
            preliminary_steps.extend(build_order.preliminary_steps)

        self.build.addStepsAfterCurrentStep(
            [ExecuteTriggerStages(build_orders, **self.kwargs)])
        self.build.addStepsAfterCurrentStep(preliminary_steps)

        return succeed(SUCCESS)


class BaseBuildOrder(object):  # pylint:disable=too-few-public-methods
    """Base class representing a build to trigger
    (Scheduler and properties)
    """
    scheduler = None

    def __init__(self, stage_name, conf, parent_step):
        self._stage = conf['stages'][stage_name]
        self._worker = self._stage['worker']
        self._parent_step = parent_step
        self.properties = dict((key, value) for key, value, origin in
                               parent_step.getProperties().asList())
        self.properties['stage_name'] = stage_name
        self.properties['reason'] = stage_name
        self.properties['git_reference'] = GIT_CACHE_DIR_CONTAINER
        self.preliminary_steps = []
        self.setup_properties()

    def setup_properties(self):
        """
        Setup additional properties
        :return: None
        """
        raise NotImplementedError()

# pylint: disable=too-few-public-methods


class DockerBuildOrder(BaseBuildOrder):
    """Base class representing a build to trigger on a Docker container
    (Scheduler, properties and docker config)
    """
    scheduler = DOCKER_SCHEDULER_NAME
    docker_build_lock = MasterLock("docker_build")

    def setup_properties(self):
        docker_path = self._worker['path']
        full_docker_path = '%s/build/%s' % (
            self.properties['master_builddir'], docker_path)

        self.properties['docker_volumes'] = self._worker.get('volumes', [])
        self.properties['docker_volumes'].append(
            '%s:%s:ro' % (GIT_CACHE_DIR_HOST, GIT_CACHE_DIR_CONTAINER))
        self.properties['docker_volumes'].append(
            '%s:%s' % ('/var/run/docker.sock', '/var/run/docker.sock'))

        self.properties['docker_volumes'].append(
            '%s:%s' % (GIT_KEY_PATH, '/root/.ssh/id_rsa'))
        self.properties['docker_volumes'].append(
            '%s:%s' % (GIT_KEY_PATH + '.pub', '/root/.ssh/id_rsa.pub'))

        self.properties['docker_image'] = '%s-%06d' % (docker_path,
                                                       randint(0, 999999))

        self.preliminary_steps.append(MasterShellCommand(
            name=str('build docker image from %s' % docker_path),
            command='docker build -t %s .' % self.properties['docker_image'],
            workdir=full_docker_path,
            locks=[self.docker_build_lock.access('exclusive')],
        ))


class OpenStackBuildOrder(BaseBuildOrder):
    """Base class representing a build to trigger on an OpenStack instance
    (Scheduler, properties and OpenStack config)
    """
    scheduler = OPENSTACK_SCHEDULER_NAME

    def setup_properties(self):
        self.properties[
            'openstack_image'] = '1d3ea64f-1ead-4042-8cb6-8ceb523b6149'
        # https://developer.rackspace.com/docs/cloud-servers/v2/general-api-info/flavors/
        self.properties['openstack_flavor'] = 'general1-8'


class ExecuteTriggerStages(Trigger):
    """Allows to give specific parameter to every scheduler.

    This is a step that allows to start with the properties specified in the
    schedulerNames argument (tuple) instead of using the properties given in
    the set_properties/copy_properties parameters.
    """

    def __init__(self, build_orders, **kwargs):
        self._build_orders = build_orders
        if 'name' not in kwargs:
            kwargs['name'] = 'trigger %d stage(s)' % len(build_orders)
        Trigger.__init__(self, schedulerNames=['dummy'], **kwargs)

    def getSchedulersAndProperties(self):   # NOQA flake8 to ignore camelCase
        return [(build_order.scheduler, build_order.properties)
                for build_order in self._build_orders]


# #########################
# Bootstrap Sequence: Schedulers
# #########################
EVE_CONF['schedulers'] = []
EVE_CONF['schedulers'].append(AnyBranchScheduler(
    name=BOOTSTRAP_SCHEDULER_NAME,
    treeStableTimer=5,
    builderNames=[BOOTSTRAP_BUILDER_NAME]))

EVE_CONF['schedulers'].append(ForceScheduler(
    name='force-%s' % MASTER_NAME,
    builderNames=[BOOTSTRAP_BUILDER_NAME]))

EVE_CONF['schedulers'].append(Try_Userpass(
    name='try-%s' % MASTER_NAME,
    port=environ['TRY_PORT'],
    userpass=[("try", TRY_PWD)],
    builderNames=[BOOTSTRAP_BUILDER_NAME]))

# #########################
# Bootstrap Sequence: Build step factory
# #########################
BOOTSTRAP_FACTORY = BuildFactory()
if RAX_LOGIN:
    BOOTSTRAP_FACTORY.addStep(
        CloudfilesAuthenticate())

# Check out the source
GIT_CACHE_UPDATE_LOCK = MasterLock("git_cache_update")
BOOTSTRAP_FACTORY.addStep(
    ShellCommand(name='update git repo cache',
                 workdir=GIT_CACHE_DIR_HOST,
                 command='git clone --mirror --recursive %s . || '
                         'git remote update' % GIT_REPO,
                 locks=[GIT_CACHE_UPDATE_LOCK.access('exclusive')],
                 haltOnFailure=True))

BOOTSTRAP_FACTORY.addStep(CancelNonTipBuild())

BOOTSTRAP_FACTORY.addStep(
    SetProperty(
        name='setting the master_builddir property',
        property='master_builddir',
        value=Property('builddir')))

BOOTSTRAP_FACTORY.addStep(
    Git(name='checkout git branch',
        repourl=GIT_CACHE_DIR_HOST,
        shallow=True,
        retry=(60, 10),
        submodules=True,
        mode='incremental',
        haltOnFailure=True))
# Read conf from yaml file
BOOTSTRAP_FACTORY.addStep(ReadConfFromYaml())

# #########################
# Bootstrap Sequence: Builders
# #########################
EVE_CONF['builders'] = []
EVE_CONF['builders'].append(
    BuilderConfig(
        name=BOOTSTRAP_BUILDER_NAME,
        workernames=[lw.name for lw in LOCAL_WORKERS],
        factory=BOOTSTRAP_FACTORY))

# #########################
# Triggerable Sequence: Schedulers
# #########################
EVE_CONF['schedulers'].append(Triggerable(
    name=DOCKER_SCHEDULER_NAME,
    builderNames=[DOCKER_BUILDER_NAME]))

EVE_CONF['schedulers'].append(Triggerable(
    name=OPENSTACK_SCHEDULER_NAME,
    builderNames=[OPENSTACK_BUILDER_NAME]))

# #########################
# Triggerable Sequence: Builders
# #########################
for _builder, workers in (
        (DOCKER_BUILDER_NAME, DOCKER_WORKERS),
        (OPENSTACK_BUILDER_NAME, OPENSTACK_WORKERS)):
    factory = BuildFactory()
    factory.addStep(CancelOldBuild())
    # Extract steps from conf
    factory.addStep(StepExtractor(name='extract steps from yaml'))
    EVE_CONF['builders'].append(
        BuilderConfig(
            name=_builder,
            workernames=[w.name for w in workers],
            factory=factory,
            collapseRequests=False,
        ))

# #########################
# Collapsing requests
# #########################
EVE_CONF['collapseRequests'] = False

# #########################
# Hacks/Bugfixes
# #########################
# Hack to fix a bug stating that LocalWorkers do not have a valid path_module
for w in EVE_CONF['workers']:
    w.path_module = namedModule("posixpath")


# Compressed logs generate a lot of issues like this :
# 2016-08-28 14:17:22+0000 [-] /root/eve/workspaces/ring/venv/local/lib/
# python2.7/site-packages/sqlalchemy/engine/default.py:451:
# _mysql_exceptions.Warning: Invalid utf8 character string: 'DAC554'
# This hack fixes them by disabling log compression
EVE_CONF['logCompressionMethod'] = 'raw'


class TempSourceStamp(object):  # pylint: disable=too-few-public-methods
    """ This is a Hack to fix a bug where the git diff is sent as an str
    instead of unicode and triggers an exception
    """

    def asDict(self):  # pylint: disable=invalid-name,missing-docstring
        result = vars(self).copy()
        del result['ssid']
        del result['changes']
        if 'patch' in result and result['patch'] is None:
            result['patch'] = (None, None, None)
        result['patch_level'], result['patch_body'], result[
            'patch_subdir'] = result.pop('patch')
        result['patch_author'], result[
            'patch_comment'] = result.pop('patch_info')
        assert all(
            isinstance(val, (str, unicode, type(None), int))  # added str here
            for attr, val in result.items()
        ), result
        return result

buildrequest.TempSourceStamp = TempSourceStamp

# #########################
# Sentry Logging
# #########################


def init_sentry_logging():
    """Start logging of all failure to sentry."""
    client = Client(SENTRY_DSN,
                    transport=TwistedHTTPTransport,
                    auto_log_stacks=True)

    @provider(ILogObserver)
    def log_to_sentry(event):
        """Log a failure to Sentry."""
        if not event.get('isError') or 'failure' not in event:
            return
        failure = event['failure']
        client.captureException((failure.type, failure.value,
                                 failure.getTracebackObject()))

    globalLogPublisher.addObserver(log_to_sentry)

if SENTRY_DSN:
    init_sentry_logging()

# #########################
# Utils
# #########################


def replace_with_interpolate(obj):
    """Interpolate nested %(prop:obj)s in step arguments.

    Read step arguments from the yaml file and replaces them with
    interpolate objects when relevant so they can be replaced with
    properties when run.
    """

    if isinstance(obj, dict):
        return {k: replace_with_interpolate(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [replace_with_interpolate(elem) for elem in obj]
    elif isinstance(obj, str) and 'prop:' in obj:
        return Interpolate(obj)
    elif isinstance(obj, unicode) and 'prop:' in obj:
        return Interpolate(obj)
    else:
        return obj

# vim: ft=python
